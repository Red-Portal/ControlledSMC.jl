var documenterSearchIndex = {"docs":
[{"location":"kernel_adaptation/#Kernel-Adaptation","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"","category":"section"},{"location":"kernel_adaptation/#General-Demo","page":"Kernel Adaptation","title":"General Demo","text":"We will use the Brownian motion example.\n\nprob    = BrownianMotion()\nd       = LogDensityProblems.dimension(prob)\nprob_ad = ADgradient(AutoMooncake(; config=Mooncake.Config()), prob; x = randn(d))\nnothing\n\nLet's setup the SMC annealing path.\n\nn_iters  = 64\nproposal = MvNormal(Zeros(d), I)\nschedule = range(0, 1; length=n_iters)\npath     = GeometricAnnealingPath(schedule, proposal, prob_ad)\nnothing\n\nWe can use the incremental KL divergence minimizing adaptation by constructing the SMC sampler as follows:\n\nsampler = SMCULA(path, BackwardKLMin(n_subsample=128, regularization=0.1))\nnothing\n\nThen, adaptation is performed during SMC sampling:\n\nn_particles = 1024\n_, _, sampler, _, info = ControlledSMC.sample(\n    sampler, n_particles, 0.5; show_progress=false\n)\nnothing\n\nsample returns a sampler object with the adapted parameters:\n\nPlots.plot(sampler.stepsizes, xlabel=\"SMC Iteration\", ylabel=\"Stepsize h\")\nsavefig(\"ula_stepsizes.svg\")\nnothing\n\n(Image: )\n\nNote that the log normalizer computed during the adapted SMC run:\n\nlast(info).log_normalizer\n\nis a biased estimate due to adaptation. Therefore, to obtain unbiased estimates, it is necessary to run SMC once again with the adapted parameters:\n\n_, _, _, _, info = ControlledSMC.sample(\n    (@set sampler.adaptor = nothing), n_particles, 0.5; show_progress=false\n)\nlast(info).log_normalizer\n\nThe biased log normalizer estimates will probably overestimate the normalizing constant by a margin decreasing with n_particles.","category":"section"},{"location":"kernel_adaptation/#Metropolis-Hastings-Adjusted-Kernels","page":"Kernel Adaptation","title":"Metropolis-Hastings-Adjusted Kernels","text":"We can also adapt the stepsizes of Metropolis-Hastings-adjusted kernels such as MALA. For instance, we can control the acceptance rate to follow the well-known optimal scaling heuristics:\n\nsampler = SMCMALA(\n    path, \n    AcceptanceRateControl(\n        n_subsample=128, \n        regularization=0.1, \n        target_acceptance_rate=0.574\n   )\n)\n_, _, sampler, _, _ = ControlledSMC.sample(\n    sampler, n_particles, 0.5; show_progress=false\n)\nPlots.plot(sampler.stepsizes, xlabel=\"SMC Iteration\", ylabel=\"Stepsize h\")\nsavefig(\"mala_acc_stepsizes.svg\")\nnothing\n\n(Image: )","category":"section"},{"location":"kernel_adaptation/#Unadjusted-Hamiltonian-Monte-Carlo","page":"Kernel Adaptation","title":"Unadjusted Hamiltonian Monte Carlo","text":"Lastly, we can also adapt unadjusted generalized Hamiltonian Monte Carlo (ugHMC). Unlike ULA, ugHMC has two parameters: the stepsize and the refreshment rate. We tune both parameters in a coordinate-descent scheme, where the refreshment rate is optimized by an grid search. Let's first see the results under the default grid [0.1, 0.9].\n\nsampler = SMCUHMC(path, BackwardKLMin(n_subsample=128, regularization=5.0))\n\n_, _, sampler, _, _ = ControlledSMC.sample(\n    sampler, n_particles, 0.5; show_progress=false\n)\n\nPlots.plot(hcat(sampler.stepsizes, sampler.refresh_rates), layout=2, xlabel=\"SMC Iteration\")\nsavefig(\"uhmc_params.svg\")\nnothing\n\n(Image: )\n\nNow, the default grid is essentially a set of binary decisions: to refresh the momentum or to not refresh the momentum. This is a close approximation of the solution obtained under a finer grid:\n\nsampler = SMCUHMC(path, BackwardKLMin(n_subsample=128, regularization=5.0), refresh_rate_grid=range(0.1, 0.9; length=16))\n\n_, _, sampler, _, _ = ControlledSMC.sample(\n    sampler, n_particles, 0.5; show_progress=false\n)\n\nPlots.plot(hcat(sampler.stepsizes, sampler.refresh_rates), layout=2, xlabel=\"SMC Iteration\", label=[\"stepsize\" \"refreshment rate\"])\nsavefig(\"uhmc_params_fine_grid.svg\")\nnothing\n\n(Image: )\n\nSee the distribution of the adapted refreshment rates:\n\nPlots.histogram(sampler.refresh_rates, bins=range(0,1; length=16), normed=true)\nsavefig(\"uhmc_refresh_rates.svg\")\nnothing\n\n(Image: )\n\nMost of them are concentrated on the extreme solutions. Therefore, a coarse grid like [0.1, 0.9] is an effective approximation.","category":"section"},{"location":"#ControlledSMC","page":"Home","title":"ControlledSMC","text":"Documentation for ControlledSMC.\n\n","category":"section"},{"location":"#ControlledSMC.ssp_sampling","page":"Home","title":"ControlledSMC.ssp_sampling","text":"ssp_sampling\n\nSSP stands for Srinivasan Sampling Process. This resampling scheme is discussed in Gerber et al.[GCW2019]. Basically, it has similar properties as systematic resampling (number of off-springs is either k or k + 1, with k <= N W^n < k +1), and in addition is consistent. See that paper for more details.\n\nReference\n\n[GCW2019]: Gerber M., Chopin N. and Whiteley N. (2019). Negative association, ordering and convergence of resampling methods. The Annals of Statistics 47 (2019), no. 4, 2236â€“2260.\n\n\n\n\n\n","category":"function"}]
}
