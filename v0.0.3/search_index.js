var documenterSearchIndex = {"docs":
[{"location":"kernel_adaptation/#Kernel-Adaptation","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"","category":"section"},{"location":"kernel_adaptation/#General-Demo","page":"Kernel Adaptation","title":"General Demo","text":"","category":"section"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"using ADTypes, DifferentiationInterface, Mooncake\nusing Accessors\nusing ControlledSMC\nusing Distributions\nusing FillArrays\nusing LinearAlgebra\nusing LogDensityProblems, LogDensityProblemsAD\nusing Plots, StatsPlots\nusing Random\n\ninclude(\"brownian.jl\")","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"We will use the Brownian motion example.","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"prob    = BrownianMotion()\nd       = LogDensityProblems.dimension(prob)\nprob_ad = ADgradient(AutoMooncake(; config=Mooncake.Config()), prob; x = randn(d))\nnothing","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"Let's setup the SMC annealing path.","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"n_iters  = 64\nproposal = MvNormal(Zeros(d), I)\nschedule = range(0, 1; length=n_iters)\npath     = GeometricAnnealingPath(schedule, proposal, prob_ad)\nnothing","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"We can use the incremental KL divergence minimizing adaptation by constructing the SMC sampler as follows:","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"sampler = SMCULA(path, BackwardKLMin(n_subsample=128, regularization=0.1))\nnothing","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"Then, adaptation is performed during SMC sampling:","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"n_particles = 1024\n_, _, sampler, _, info = ControlledSMC.sample(\n    sampler, n_particles, 0.5; show_progress=false\n)\nnothing","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"sample returns a sampler object with the adapted parameters:","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"Plots.plot(sampler.stepsizes, xlabel=\"SMC Iteration\", ylabel=\"Stepsize h\")\nsavefig(\"ula_stepsizes.svg\")\nnothing","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"(Image: )","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"Note that the log normalizer computed during the adapted SMC run:","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"last(info).log_normalizer","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"is a biased estimate due to adaptation. Therefore, to obtain unbiased estimates, it is necessary to run SMC once again with the adapted parameters:","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"_, _, _, _, info = ControlledSMC.sample(\n    (@set sampler.adaptor = nothing), n_particles, 0.5; show_progress=false\n)\nlast(info).log_normalizer","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"The biased log normalizer estimates will probably overestimate the normalizing constant by a margin decreasing with n_particles.","category":"page"},{"location":"kernel_adaptation/#Metropolis-Hastings-Adjusted-Kernels","page":"Kernel Adaptation","title":"Metropolis-Hastings-Adjusted Kernels","text":"","category":"section"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"We can also adapt the stepsizes of Metropolis-Hastings-adjusted kernels such as MALA. For instance, we can control the acceptance rate to follow the well-known optimal scaling heuristics:","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"sampler = SMCMALA(\n    path, \n    AcceptanceRateControl(\n        n_subsample=128, \n        regularization=0.1, \n        target_acceptance_rate=0.574\n   )\n)\n_, _, sampler, _, _ = ControlledSMC.sample(\n    sampler, n_particles, 0.5; show_progress=false\n)\nPlots.plot(sampler.stepsizes, xlabel=\"SMC Iteration\", ylabel=\"Stepsize h\")\nsavefig(\"mala_acc_stepsizes.svg\")\nnothing","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"(Image: )","category":"page"},{"location":"kernel_adaptation/#Unadjusted-Hamiltonian-Monte-Carlo","page":"Kernel Adaptation","title":"Unadjusted Hamiltonian Monte Carlo","text":"","category":"section"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"Lastly, we can also adapt unadjusted generalized Hamiltonian Monte Carlo (ugHMC). Unlike ULA, ugHMC has two parameters: the stepsize and the refreshment rate. We tune both parameters in a coordinate-descent scheme, where the refreshment rate is optimized by an grid search. Let's first see the results under the default grid [0.1, 0.9].","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"sampler = SMCUHMC(path, BackwardKLMin(n_subsample=128, regularization=5.0))\n\n_, _, sampler, _, _ = ControlledSMC.sample(\n    sampler, n_particles, 0.5; show_progress=false\n)\n\nPlots.plot(hcat(sampler.stepsizes, sampler.refresh_rates), layout=2, xlabel=\"SMC Iteration\")\nsavefig(\"uhmc_params.svg\")\nnothing","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"(Image: )","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"Now, the default grid is essentially a set of binary decisions: to refresh the momentum or to not refresh the momentum. This is a close approximation of the solution obtained under a finer grid:","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"sampler = SMCUHMC(path, BackwardKLMin(n_subsample=128, regularization=5.0), refresh_rate_grid=range(0.1, 0.9; length=16))\n\n_, _, sampler, _, _ = ControlledSMC.sample(\n    sampler, n_particles, 0.5; show_progress=false\n)\n\nPlots.plot(hcat(sampler.stepsizes, sampler.refresh_rates), layout=2, xlabel=\"SMC Iteration\", label=[\"stepsize\" \"refreshment rate\"])\nsavefig(\"uhmc_params_fine_grid.svg\")\nnothing","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"(Image: )","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"See the distribution of the adapted refreshment rates:","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"Plots.histogram(sampler.refresh_rates, bins=range(0,1; length=16), normed=true)\nsavefig(\"uhmc_refresh_rates.svg\")\nnothing","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"(Image: )","category":"page"},{"location":"kernel_adaptation/","page":"Kernel Adaptation","title":"Kernel Adaptation","text":"Most of them are concentrated on the extreme solutions. Therefore, a coarse grid like [0.1, 0.9] is an effective approximation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = ControlledSMC","category":"page"},{"location":"#ControlledSMC","page":"Home","title":"ControlledSMC","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for ControlledSMC.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [ControlledSMC]","category":"page"},{"location":"#ControlledSMC.ssp_sampling","page":"Home","title":"ControlledSMC.ssp_sampling","text":"ssp_sampling\n\nSSP stands for Srinivasan Sampling Process. This resampling scheme is discussed in Gerber et al.[GCW2019]. Basically, it has similar properties as systematic resampling (number of off-springs is either k or k + 1, with k <= N W^n < k +1), and in addition is consistent. See that paper for more details.\n\nReference\n\n[GCW2019]: Gerber M., Chopin N. and Whiteley N. (2019). Negative association, ordering and convergence of resampling methods. The Annals of Statistics 47 (2019), no. 4, 2236â€“2260.\n\n\n\n\n\n","category":"function"}]
}
